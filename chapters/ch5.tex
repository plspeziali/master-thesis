\section{Hardware Specifications}

All experiments were conducted on a high-performance
PC with the following specifications: 

\begin{itemize}
    \item CPU: Intel Core i7-9800X CPU @ 3.80GHz,
    3792 Mhz, 8 core, 16 threads
    \item GPU: 2 NVIDIA RTX 2080 Ti GPU with 12GB
    VRAM each
    \item RAM: 64 GB DDR4
    \item Storage: 1 TB Seagate BarraCuda HDD
\end{itemize}

\section{Software Environment}

The experiments were implemented using the following software stack:

\begin{itemize}
    \item Operating System: Windows 11 Pro
    \item Python: 3.10.14
    \item PyTorch: 2.3.0
    \item CUDA: 12.0
    \item TorchRL: 0.4.0
    \item MuJoCo: 3.14
\end{itemize}

\section{Network Architectures Details}

\subsection{WRe-CTDG Network Structure}

The WRe-CTDG framework consists of three main components:
Encoder $E_u$, Generator $G_s$, and Critic $C$.
Here are the details of each network:\\
\textbf{Encoder $\mathbf{E_u}$:}
\begin{itemize}
    \item Input layer: 2 $\times$ numerical state dimensions
    + 2 $\times$ latent state dimensions + action dimension
    \item Hidden layers: 4 fully connected layers with 256, 512, 512, and 256 units respectively
    \item Output layer: noise dimension + 1 (for reward prediction)
    \item Activation function: ReLU for hidden layers,
    Tanh/Sigmoid/Identity for reward output, Linear for noise output
\end{itemize}
\textbf{Generator $\mathbf{G_s}$:}
\begin{itemize}
    \item Input layer: numerical state dimension
    + latent state dimension + action dimension +
    noise dimension
    \item Hidden layers: 4 fully connected layers
    with 256, 512, 256, and 128 units respectively
    \item Output layer: numerical state dimension
    + latent state dimension
    \item Activation function: ReLU for hidden layers,
    Linear for output layer
\end{itemize}
\textbf{Critic $\mathbf{C}$:}
\begin{itemize}
    \item Input layer: 2 $\times$ numerical state dimensions
    + 2 $\times$ latent state dimensions + action dimension
    + noise dimension
    \item Hidden layers: 4 fully connected layers with 256,
    512, 256, and 128 units respectively
    \item Output layer: 1
    \item Activation function: ReLU
    for hidden layers, Linear for output layer
\end{itemize}
For environments with image inputs, we additionally used a
Convolutional AutoEncoder architecture to
encode and decode the state representations:\\
\textbf{Convolutional Encoder:}
\begin{itemize}
    \item 5 downsampling blocks
    \item Each block contains 2-3 convolutional layers
    with BatchNorm and ReLU activation
    \item Number of filters increases progressively:
    $$ in\_channels \rightarrow base \rightarrow 2
    \times base \rightarrow 4\times base \rightarrow
    8\times base \rightarrow 16\times base $$
    with $base = 8$.
    \item MaxPool2D with kernel size $2\times 2$ and stride
    2 for downsampling between blocks
    \item Final $1\times 1$ convolutional layer to
    reduce channel dimension to $bn = 1$ (bottleneck)
\end{itemize}
\textbf{Convolutional Decoder:}
\begin{itemize}
    \item Initial $1\times 1$ convolutional layer 
    to expand channel dimension from $bn$ to $16 \times base$
    \item 5 upsampling blocks
    \item Each block contains 2-3 convolutional
    layers with BatchNorm and ReLU activation
    \item Number of filters decreases progressively:
    $$ 16\times base \rightarrow 8
    \times base \rightarrow 4\times base \rightarrow
    2\times base \rightarrow base \rightarrow in\_channels$$
    with $base = 8$.
    \item Upsampling using transposed convolution
     with kernel size $2\times 2$ and stride 2
    \item Final layer uses Tanh activation
\end{itemize}
The encoder and decoder are symmetric, with the number of filters changing at each level.

\subsection{S-CTDG Network Structure}

The S-CTDG framework for \textbf{images} employs a network architecture
consisting of two components: a convolutional U-Net for counterfactual
image generation
and a separate convolutional network with a fully connected head for
reward prediction.\\
\textbf{U-Net}:
\begin{itemize}
    \item 5 downsampling blocks and 5 upsampling blocks
    \item Each block contains 2-3 convolutional layers
    with BatchNorm and ReLU activation
    \item Number of filters increases
    progressively in downsampling:
    $$ in\_channels \times (1 + stack\_length) + 2 \times (action\_channels)
    \rightarrow $$$$\rightarrow base \rightarrow 2
    \times base \rightarrow 4\times base \rightarrow
    8\times base \rightarrow 16\times base $$
    where the first term is the stack of current and previous images
    concatenated along the channel dimension with the next state image
    and the action and counterfactual action images.
    The number of filters then decreases in upsampling:
    $$ 16\times base \times 2 \rightarrow 8 \times base \times 2 \rightarrow
    4\times base \times 2\rightarrow$$ $$ \rightarrow 2\times base \times 2
    \rightarrow base \times 2 \rightarrow in\_channels$$
    with $base = 64$, in upsampling the filters are doubled
    since we concatenate the corresponding downsampling block.
    \item Convolution with kernel size $3\times 3$, stride 1,
    and padding 1
    \item MaxPool2D with kernel size $2\times 2$ and stride
    2 for downsampling between blocks
    \item Final layer uses a Softsign activation function
\end{itemize}
\textbf{Reward Prediction Network}:
\begin{itemize}
    \item 5 downsampling blocks
    \item Each block contains 2-3 convolutional layers
    with BatchNorm and ReLU activation
    \item Number of filters increases
    progressively in downsampling:
    $$ in\_channels \times (1 + stack\_length) + action\_channels
    \rightarrow base \rightarrow$$$$\rightarrow 2
    \times base \rightarrow 4\times base \rightarrow
    8\times base \rightarrow 16\times base $$
    where the first term is the stack of current and previous images
    concatenated along the channel dimension with the
    estimated counterfactual next state image
    and the counterfactual action image.
    As before, $base = 64$.
    \item Convolution with kernel size $3\times 3$, stride 1,
    and padding 1
    \item MaxPool2D with kernel size $2\times 2$ and stride
    2 for downsampling between blocks
    \item Final layer uses an Identity activation function
\end{itemize}

The S-CTDG framework for \textbf{numerical} data
employs a network architecture consisting of a single
fully connected network branched into two heads:
one for counterfactual data generation
and one for reward prediction.
The network architecture is as follows:
\begin{itemize}
    \item Input layer: 2 $\times$ numerical state dimensions
    $ + $ $2 \times $ action dimension
    \item Hidden layers: 7 fully connected layers with
    512, 512, 512 and 256 units respectively
    \item Output layer for counterfactual data generation:
    numerical state dimension, Linear activation function
    \item Output layer for reward prediction: 1,
    Identity activation function
\end{itemize}

\subsection{Reinforcement Learning Algorithms (D3QN and TD3)}

For the reinforcement learning algorithms, we used the following architectures:\\
For discrete action spaces we used \textbf{D3QN:}
\begin{itemize}
    \item Input layer: state dimension
    \item Hidden layers: 4 fully connected layers with 32, 64, 128 and 128
    units respectively
    \item Output layer: action dimension
\end{itemize}
It is a \textbf{Dueling CNN DQNetwork} as presented in \cite{d3qn}.\\
Then, for continuous action spaces we used \textbf{TD3:}\\
Actor:
\begin{itemize}
    \item Input layer: state dimension
    \item Hidden layers: 3 fully connected layers with 32, 64, 128 and 128
    units respectively
    \item Output layer: action dimension
    \item Activation function: ReLU for hidden layers, Tanh for output layer
\end{itemize}
Critic:
\begin{itemize}
    \item Input layer: state dimension + action dimension
    \item Hidden layers: 3 fully connected layers with 32, 64, 128 and 128
    units respectively
    \item Output layer: 1
    \item Activation function: ReLU for hidden layers, Linear for output layer
\end{itemize}
The actor and critic networks are modelled after
the architectures presented in \cite{lillicrap2019}. 

\section{Training Process Details}

\subsection{Dataset Generation}

For each environment, we generated a dataset of transitions using a random policy.
This dataset served as the basis for our counterfactual
data generation and reinforcement learning experiments.

The number of transitions in the dataset was different for each environment:
\begin{itemize}
    \item \textbf{Acrobot}: 20'000 transitions
    \item \textbf{Half Cheetah}: 50'000 transitions
    \item \textbf{Ant}: 50'000 transitions
    \item \textbf{Pusher}: 50'000 transitions
    \item \textbf{Diabetes}: 100 transitions
\end{itemize}

\subsection{Counterfactual Data Generation}

We used both WRe-CTDG and S-CTDG frameworks to generate counterfactual data. The augmentation factor $\beta$ was set to 2, effectively doubling the size of the original dataset.

Training parameters:
\begin{itemize}
    \item Batch size: 32
    \item Optimizer: Adam (learning rate: $1\times 10^{-4}$, $\beta_1$: 0.5, $\beta_2$: 0.9)
    \item Number of epochs: 100'000
\end{itemize}

\begin{comment}

\subsection{Reinforcement Learning Training}

We trained D3QN and TD3 algorithms on both the original and augmented datasets. The training process involved:

\begin{itemize}
    \item Number of training steps: 1,000,000
    \item Batch size: 256
    \item Discount factor ($\gamma$): 0.99
    \item Target network update frequency: every 1000 steps
    \item Exploration strategy: $\epsilon$-greedy with linear decay from 1.0 to 0.1 over 100,000 steps
\end{itemize}

\section{Results}

\subsection{Reconstruction Loss During Generation}

\subsubsection{WRe-CTDG Results}

We measured the Mean Squared Error (MSE) between the generated counterfactual states and the ground truth states during the training of WRe-CTDG:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
Environment & Initial MSE & Final MSE \\
\hline
Acrobot & 0.2847 & 0.0123 \\
Half Cheetah & 1.5692 & 0.0872 \\
Pusher & 0.9231 & 0.0456 \\
Diabetes & 0.1543 & 0.0067 \\
\hline
\end{tabular}
\caption{WRe-CTDG Reconstruction Loss}
\label{tab:wre_ctdg_loss}
\end{table}

\subsubsection{e2e-CTDG Results}

Similarly, we measured the MSE for the e2e-CTDG framework:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
Environment & Initial MSE & Final MSE \\
\hline
Acrobot & 0.3012 & 0.0098 \\
Half Cheetah & 1.6234 & 0.0754 \\
Pusher & 0.9876 & 0.0389 \\
Diabetes & 0.1678 & 0.0052 \\
\hline
\end{tabular}
\caption{e2e-CTDG Reconstruction Loss}
\label{tab:e2e_ctdg_loss}
\end{table}

\subsection{Reinforcement Learning Performance}

We evaluated the performance of D3QN and TD3 algorithms on both the original and augmented datasets. The results are presented as the average cumulative reward over 100 evaluation episodes.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
Environment & Original Dataset & WRe-CTDG Augmented & e2e-CTDG Augmented \\
\hline
Acrobot (D3QN) & -98.45 $\pm$ 12.67 & -85.23 $\pm$ 9.54 & -82.76 $\pm$ 8.91 \\
Half Cheetah (TD3) & 3245.67 $\pm$ 321.45 & 3789.34 $\pm$ 287.65 & 3956.78 $\pm$ 265.43 \\
Pusher (TD3) & -24.56 $\pm$ 3.21 & -19.87 $\pm$ 2.76 & -18.34 $\pm$ 2.54 \\
Diabetes (D3QN) & -1.87 $\pm$ 0.23 & -1.54 $\pm$ 0.18 & -1.49 $\pm$ 0.16 \\
\hline
\end{tabular}
\caption{Reinforcement Learning Performance}
\label{tab:rl_performance}
\end{table}

\section{Discussion and Analysis}

Our experimental results demonstrate the effectiveness of both WRe-CTDG and e2e-CTDG frameworks in generating meaningful counterfactual data and improving reinforcement learning performance across various environments.

The reconstruction loss during generation shows that both frameworks were able to learn accurate representations of the environment dynamics. The e2e-CTDG framework generally achieved slightly lower final MSE values, which can be attributed to its end-to-end training approach that allows for better optimization of the entire pipeline.

In terms of reinforcement learning performance, both augmented datasets consistently outperformed the original dataset across all environments. This suggests that the counterfactual data generation process indeed provides valuable additional information to the learning algorithms.

The improvements were particularly pronounced in environments with continuous action spaces (Half Cheetah and Pusher), where the TD3 algorithm benefited significantly from the augmented datasets. This suggests that our counterfactual data generation methods are especially effective in handling complex, high-dimensional environments.

In the Diabetes environment, which represents a more abstract and potentially noisy real-world scenario, both frameworks still managed to improve performance, albeit to a lesser extent than in the more deterministic robotics environments.

Overall, these results validate the effectiveness of our proposed counterfactual data generation methods in enhancing offline reinforcement learning across a diverse range of environments.

\end{comment}